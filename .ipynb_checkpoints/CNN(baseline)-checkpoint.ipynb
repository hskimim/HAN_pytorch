{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import ijson\n",
    "import pandas as pd\n",
    "from collections import Counter,defaultdict\n",
    "from imblearn.over_sampling import *\n",
    "from bs4 import BeautifulSoup\n",
    "import itertools\n",
    "\n",
    "import pickle\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx_dict = load_obj('word_to_idx_dict')\n",
    "word_to_freq_dict = load_obj('word_to_freq_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = torch.load('train_X.pt')\n",
    "train_y = torch.load('train_y.pt')\n",
    "test_X = torch.load('test_X.pt')\n",
    "test_y = torch.load('test_y.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        'Initialization'\n",
    "        self.y = y\n",
    "        self.X = X\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load data and get label\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 100,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 10}\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(train_X,train_y)\n",
    "train_iter = data.DataLoader(training_set, **params)\n",
    "\n",
    "testing_set = Dataset(test_X,test_y)\n",
    "test_iter = data.DataLoader(testing_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for local_batch, local_labels in train_iter:\n",
    "    local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 12, 25]), torch.Size([100]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_batch.size(),local_labels.size()\n",
    "#[batch_size, sent_len, word_len]\n",
    "# 25개의 단어를 가지고 12개의 문장을 가진 64개의 문서가 있는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module) : \n",
    "    \n",
    "    def __init__(self,VOCAB_SIZE, EMBED_SIZE, HID_SIZE, DROPOUT, BATCH_SIZE ,KERNEL_SIZE, NUM_FILTER, N_CLASS ) : \n",
    "        super(CNN, self).__init__()\n",
    "        self.vocab_size = VOCAB_SIZE \n",
    "        self.embed_size = EMBED_SIZE \n",
    "        self.hid_size = HID_SIZE \n",
    "        self.dropout = DROPOUT \n",
    "        self.batch_size = BATCH_SIZE\n",
    "        if type(KERNEL_SIZE) !=list :\n",
    "            self.kernel_size = list(KERNEL_SIZE)\n",
    "        else : self.kernel_size = KERNEL_SIZE \n",
    "        self.num_filter = NUM_FILTER \n",
    "        self.num_class = N_CLASS \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings = self.vocab_size,\n",
    "            embedding_dim = self.embed_size,\n",
    "            padding_idx = 1) \n",
    "        \n",
    "        self.convs = nn.ModuleList([(nn.Conv2d(in_channels = 1,out_channels = self.num_filter,\\\n",
    "        kernel_size = (kernel,self.embed_size))) for kernel in self.kernel_size])\n",
    "        \n",
    "        self.fully_connect = nn.Sequential(\n",
    "        nn.Linear(self.num_filter * len(self.kernel_size),self.hid_size),nn.ReLU(),\n",
    "        nn.Dropout(self.dropout),nn.Linear(self.hid_size , self.num_class),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x) : \n",
    "        x = x.view(self.batch_size,-1) # [batch_size,max_length]\n",
    "        \n",
    "        embed = self.embedding(x) \n",
    "        embed = embed.unsqueeze(1)\n",
    "                \n",
    "        convolution = [conv(embed).squeeze(3) for conv in self.convs]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv,(conv.size(2))).squeeze(2) for conv in convolution]\n",
    "        \n",
    "        dropout = [F.dropout(pool,self.dropout) for pool in pooled]\n",
    "        \n",
    "        concatenate = torch.cat(dropout, dim = 1) \n",
    "        # [batch_size , num_filter * num_kernel]\n",
    "\n",
    "        logit = self.fully_connect(concatenate)\n",
    "        \n",
    "        return torch.log_softmax(logit,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (embedding): Embedding(170007, 256, padding_idx=1)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 4, kernel_size=(2, 256), stride=(1, 1))\n",
       "    (1): Conv2d(1, 4, kernel_size=(3, 256), stride=(1, 1))\n",
       "    (2): Conv2d(1, 4, kernel_size=(4, 256), stride=(1, 1))\n",
       "    (3): Conv2d(1, 4, kernel_size=(5, 256), stride=(1, 1))\n",
       "  )\n",
       "  (fully_connect): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=128, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(word_to_idx_dict)\n",
    "EMBED_SIZE = 256\n",
    "HID_SIZE = 128\n",
    "DROPOUT = 0.5\n",
    "BATCH_SIZE = 100\n",
    "KERNEL_SIZE = [2,3,4,5]\n",
    "NUM_FILTER = 4\n",
    "N_CLASS = 5\n",
    "\n",
    "model = CNN(VOCAB_SIZE, EMBED_SIZE, HID_SIZE, DROPOUT, BATCH_SIZE, KERNEL_SIZE, NUM_FILTER, N_CLASS)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_softmax = model(local_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainig and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, init_lr=0.1, decay = 0.1 ,per_epoch=10):\n",
    "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= 1/(1 + decay)\n",
    "\n",
    "    return optimizer , float(param_group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model,train_loader , test_loader , epochs = 10, lr = 0.01, batch_size = 100) :\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr)\n",
    "    criterion = nn.NLLLoss().to(device)\n",
    "\n",
    "    for epoch in range(1,epochs+1) :\n",
    "        optimizer , lr_int = \\\n",
    "        adjust_learning_rate(optimizer, epoch, init_lr=lr, decay = 0.1 ,per_epoch=10)\n",
    "        model.train()        \n",
    "        n_correct = 0\n",
    "        batch_count = 0\n",
    "        for local_batch, local_labels in train_loader:\n",
    "            \n",
    "            batch_count += 1 \n",
    "            if batch_count % 1000 == 0 : \n",
    "                print(\"{}번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\".format(batch_count))\n",
    "                \n",
    "            local_batch,local_labels = local_batch.to(device),local_labels.to(device)\n",
    "        \n",
    "            train_softmax = model(local_batch)\n",
    "            train_predict = train_softmax.argmax(dim=1)\n",
    "\n",
    "            n_correct += (train_predict == local_labels).sum().item()            \n",
    "            loss = criterion(train_softmax,local_labels)\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        acc = n_correct / (len(train_loader) * batch_size)  \n",
    "        with open('cnn_log.txt', 'a') as f:\n",
    "            f.write('Train epoch : %s,  loss : %s,  accuracy :%.3f, learning rate :%.3f\\n\\n'%(epoch, loss.item(), acc,lr_int))\n",
    "            \n",
    "        print('Train epoch : %s,  loss : %s,  accuracy :%.3f, learning rate :%.3f'%(epoch, loss.item(), acc,lr_int))\n",
    "        print('=================================================================================================')\n",
    "        \n",
    "        if (epoch) % 2 == 0:\n",
    "            model.eval()\n",
    "            n_correct = 0  # accuracy 계산을 위해 맞은 갯수 카운트\n",
    "            val_loss = 0\n",
    "\n",
    "            for local_batch, local_labels in test_loader:\n",
    "                local_batch,local_labels = local_batch.to(device),local_labels.to(device)\n",
    "                \n",
    "                test_softmax = model(local_batch)\n",
    "                test_predict = test_softmax.argmax(dim = 1)\n",
    "\n",
    "                val_loss = criterion(test_softmax, local_labels)\n",
    "                \n",
    "                n_correct += (test_predict == local_labels).sum().item() #맞은 갯수                \n",
    "\n",
    "            val_acc = n_correct / (len(test_loader) * batch_size)\n",
    "            with open('cnn_log.txt','a') as f : \n",
    "    \n",
    "                f.write('Val Epoch : %s, Val Loss : %.03f , Val Accuracy : %.03f\\n\\n'%(epoch, val_loss, val_acc))\n",
    "                \n",
    "            print('*************************************************************************************************')\n",
    "            print('*************************************************************************************************')\n",
    "            print('Val Epoch : %s, Val Loss : %.03f , Val Accuracy : %.03f'%(epoch, val_loss, val_acc))\n",
    "            print('*************************************************************************************************')\n",
    "            print('*************************************************************************************************')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "2000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "3000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "4000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "5000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "6000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "Train epoch : 1,  loss : 1.3343491554260254,  accuracy :0.496, learning rate :0.009\n",
      "=================================================================================================\n",
      "1000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "2000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "3000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "4000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "5000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "6000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "Train epoch : 2,  loss : 1.1001183986663818,  accuracy :0.512, learning rate :0.008\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Val Epoch : 2, Val Loss : 1.054 , Val Accuracy : 0.534\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "1000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "2000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "3000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "4000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "5000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "6000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "Train epoch : 3,  loss : 1.2400468587875366,  accuracy :0.517, learning rate :0.008\n",
      "=================================================================================================\n",
      "1000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "2000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "3000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "4000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "5000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "6000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "Train epoch : 4,  loss : 1.1779104471206665,  accuracy :0.521, learning rate :0.007\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Val Epoch : 4, Val Loss : 1.198 , Val Accuracy : 0.551\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "1000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "2000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "3000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "4000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "5000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "6000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "Train epoch : 5,  loss : 1.2814298868179321,  accuracy :0.530, learning rate :0.006\n",
      "=================================================================================================\n",
      "1000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "2000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "3000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "4000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "5000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "6000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "Train epoch : 6,  loss : 1.2522425651550293,  accuracy :0.540, learning rate :0.006\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Val Epoch : 6, Val Loss : 1.116 , Val Accuracy : 0.555\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "1000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "2000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "3000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "4000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "5000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "6000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "Train epoch : 7,  loss : 1.0648549795150757,  accuracy :0.546, learning rate :0.005\n",
      "=================================================================================================\n",
      "1000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "2000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "3000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "4000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "5000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "6000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "Train epoch : 8,  loss : 1.0383000373840332,  accuracy :0.551, learning rate :0.005\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Val Epoch : 8, Val Loss : 1.004 , Val Accuracy : 0.570\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "1000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "2000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "3000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "4000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "5000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "6000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "Train epoch : 9,  loss : 1.0363081693649292,  accuracy :0.555, learning rate :0.004\n",
      "=================================================================================================\n",
      "1000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "2000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "3000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "4000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "5000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "6000번째 배치가 돌고 있습니다. 한 에포크는 6700입니다.\n",
      "Train epoch : 10,  loss : 1.0186984539031982,  accuracy :0.563, learning rate :0.004\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Val Epoch : 10, Val Loss : 0.979 , Val Accuracy : 0.570\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "train(model, train_iter, test_iter, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
